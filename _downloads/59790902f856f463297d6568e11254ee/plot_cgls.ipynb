{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# CGLS Solver\n\nThis example demonstrates the utilization of :py:func:`pylops_mpi.optimization.basic.cgls` solver.\nOur solver uses the :py:class:`pylops_mpi.DistributedArray` to reduce the following cost function\nin a distributed fashion :\n\n\\begin{align}J = \\| \\mathbf{y} -  \\mathbf{Ax} \\|_2^2 + \\epsilon \\| \\mathbf{x} \\|_2^2\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom mpi4py import MPI\nfrom matplotlib import pyplot as plt\n\nimport pylops\n\nimport pylops_mpi\n\nnp.random.seed(42)\nplt.close(\"all\")\nrank = MPI.COMM_WORLD.Get_rank()\nsize = MPI.COMM_WORLD.Get_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a matrix with dimensions ``N`` and ``M`` and populate it with\nrandom numbers. Then, we will input this matrix in a\n:py:class:`pylops_mpi.basicoperators.MPIBlockDiag`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N, M = 20, 15\nMop = pylops.MatrixMult(A=np.random.normal(0, 1, (N, M)))\nBDiag = pylops_mpi.MPIBlockDiag(ops=[Mop, ], dtype=np.float128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By applying the :py:class:`pylops_mpi.basicoperators.MPIBlockDiag` operator,\nwe perform distributed matrix-vector multiplication.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = pylops_mpi.DistributedArray(size * M, dtype=np.float128)\nx[:] = np.ones(M)\ny = BDiag @ x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now utilize the cgls solver to obtain the inverse of the ``MPIBlockDiag``.\nIn the case of MPIBlockDiag, each operator is responsible for performing\nan inversion operation iteratively at a specific rank. The resulting inversions\nare then obtained in a :py:class:`pylops_mpi.DistributedArray`. To obtain the\noverall inversion of the entire MPIBlockDiag, you can utilize the ``asarray()``\nfunction of the DistributedArray as shown below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set initial guess `x0` to zeroes\nx0 = pylops_mpi.DistributedArray(BDiag.shape[1], dtype=np.float128)\nx0[:] = 0\nxinv, istop, niter, r1norm, r2norm, cost = pylops_mpi.cgls(BDiag, y, x0=x0, niter=15, tol=1e-10, show=True)\nxinv_array = xinv.asarray()\n\nif rank == 0:\n    print(f\"CGLS Solution xinv={xinv_array}\")\n    # Visualize\n    plt.figure(figsize=(18, 5))\n    plt.plot(cost, lw=2, label=\"CGLS\")\n    plt.title(\"Cost Function\")\n    plt.legend()\n    plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}