{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Array\nThis example shows how to use the :py:class:`pylops_mpi.DistributedArray`.\nThis class provides a way to distribute arrays across multiple processes in\na parallel computing environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\nfrom mpi4py import MPI\n\nfrom pylops_mpi.DistributedArray import local_split, Partition\nimport pylops_mpi\n\nplt.close(\"all\")\nnp.random.seed(42)\n\n# MPI parameters\nsize = MPI.COMM_WORLD.Get_size()  # number of nodes\nrank = MPI.COMM_WORLD.Get_rank()  # rank of current node\n\n\n# Defining the global shape of the distributed array\nglobal_shape = (10, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by defining the class with the input parameters ``global_shape``,\n``partition``, and ``axis``. Here's an example implementation of the class\nwith ``axis=0``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arr = pylops_mpi.DistributedArray(global_shape=global_shape,\n                                  partition=pylops_mpi.Partition.SCATTER,\n                                  axis=0)\n# Filling the local arrays\narr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,\n                   arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)\npylops_mpi.plot_distributed_array(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is an implementation to show how the global array is distributed along\nthe second axis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arr = pylops_mpi.DistributedArray(global_shape=global_shape,\n                                  partition=pylops_mpi.Partition.SCATTER,\n                                  axis=1)\n# Filling the local arrays\narr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,\n                   arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)\npylops_mpi.plot_distributed_array(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You also have the option of directly including the ``local_shapes`` as a parameter\nto the :py:class:`pylops_mpi.DistributedArray`. This will enable the assignment\nof shapes to local arrays on each rank. However, it's essential to ensure that\nthe number of processes matches the length of ``local_shapes``, and that the\ncombined local shapes should align with the ``global_shape`` along the desired ``axis``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_shape = local_split(global_shape, MPI.COMM_WORLD, Partition.SCATTER, 0)\n# Assigning local_shapes(List of tuples)\nlocal_shapes = MPI.COMM_WORLD.allgather(local_shape)\narr = pylops_mpi.DistributedArray(global_shape=global_shape, local_shapes=local_shapes, axis=0)\narr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,\n                   arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)\npylops_mpi.plot_distributed_array(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convert a random NumPy array into a ``pylops_mpi.DistributedArray``,\nyou can use the ``to_dist`` classmethod. This method allows you to distribute\nthe array across multiple processes for parallel computation.\nBelow is an example implementation depicting the same.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = global_shape[0] * global_shape[1]\n# Array to be distributed\narray = np.arange(n) / float(n)\narr1 = pylops_mpi.DistributedArray.to_dist(x=array.reshape(global_shape), axis=1)\narray = array / 2.0\narr2 = pylops_mpi.DistributedArray.to_dist(x=array.reshape(global_shape), axis=1)\n# plot local arrays\npylops_mpi.plot_local_arrays(arr1, \"Distributed Array - 1\", vmin=0, vmax=1)\npylops_mpi.plot_local_arrays(arr2, \"Distributed Array - 2\", vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's move now to consider various operations that one can perform on\n:py:class:`pylops_mpi.DistributedArray` objects.\n\n**Scaling** - Each process operates on its local portion of\nthe array and scales the corresponding elements by a given scalar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scale_arr = .5 * arr1\npylops_mpi.plot_local_arrays(scale_arr, \"Scaling\", vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Element-wise Addition** - Each process operates on its local portion of\nthe array and adds the corresponding elements together.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sum_arr = arr1 + arr2\npylops_mpi.plot_local_arrays(sum_arr, \"Addition\", vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Element-wise In-place Addition** - Similar to the previous one but the\naddition is performed directly on one of the addends without creating a new\ndistributed array.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sum_arr += arr2\npylops_mpi.plot_local_arrays(sum_arr, \"Addition\", vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Element-wise Subtraction** - Each process operates on its local portion\nof the array and subtracts the corresponding elements together.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff_arr = arr1 - arr2\npylops_mpi.plot_local_arrays(diff_arr, \"Subtraction\", vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Element-wise Multiplication** - Each process operates on its local portion\nof the array and multiplies the corresponding elements together.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mult_arr = arr1 * arr2\npylops_mpi.plot_local_arrays(mult_arr, \"Multiplication\", vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's look at the case where parallelism could be applied over\nmultiple axes - and more specifically one belonging to the model/data and one\nto the operator. This kind of \"2D\"-parallelism requires repeating parts of\nthe model/data over groups of ranks. However, when global operations such as\n``dot`` or ``norm`` are applied on a ``pylops_mpi.DistributedArray`` of\nthis kind, we need to ensure that the repeated portions to do all contribute\nto the computation. This can be achieved via the ``mask`` input parameter:\na list of size equal to the number of ranks, whose elements contain the index\nof the subgroup/subcommunicator (with partial arrays in different groups\nare identical to each other).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Defining the local and global shape of the distributed array\nlocal_shape = 5\nglobal_shape = local_shape * size\n\n# Create mask\nnsub = 2\nsubsize = max(1, size // nsub)\nmask = np.repeat(np.arange(size // subsize), subsize)\nif rank == 0:\n    print(\"1D masked arrays\")\n    print(f\"Mask: {mask}\")\n\n# Create and fill the distributed array\nx = pylops_mpi.DistributedArray(global_shape=global_shape,\n                                partition=Partition.SCATTER,\n                                mask=mask)\nx[:] = (MPI.COMM_WORLD.Get_rank() % subsize + 1.) * np.ones(local_shape)\nxloc = x.asarray()\n\n# Dot product\ndot = x.dot(x)\ndotloc = np.dot(xloc[local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)],\n                xloc[local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)])\nprint(f\"Dot check (Rank {rank}): {np.allclose(dot, dotloc)}\")\n\n# Norm\nnorm = x.norm(ord=2)\nnormloc = np.linalg.norm(xloc[local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)],\n                         ord=2)\nprint(f\"Norm check (Rank {rank}): {np.allclose(norm, normloc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And with 2d-arrays distributed over axis=1\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "extra_dim_shape = 2\nif rank == 0:\n    print(\"2D masked arrays (over axis=1)\")\n\n# Create and fill the distributed array\nx = pylops_mpi.DistributedArray(global_shape=(extra_dim_shape, global_shape),\n                                partition=Partition.SCATTER,\n                                axis=1, mask=mask)\nx[:] = (MPI.COMM_WORLD.Get_rank() % subsize + 1.) * np.ones((extra_dim_shape, local_shape))\nxloc = x.asarray()\n\n# Dot product\ndot = x.dot(x)\ndotloc = np.dot(xloc[:, local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)].ravel(),\n                xloc[:, local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)].ravel())\nprint(f\"Dot check (Rank {rank}): {np.allclose(dot, dotloc)}\")\n\n# Norm\nnorm = x.norm(ord=2, axis=1)\nnormloc = np.linalg.norm(xloc[:, local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)],\n                         ord=2, axis=1)\nprint(f\"Norm check (Rank {rank}): {np.allclose(norm, normloc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally with 2d-arrays distributed over axis=0\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if rank == 0:\n    print(\"2D masked arrays (over axis=0)\")\n\n# Create and fill the distributed array\nx = pylops_mpi.DistributedArray(global_shape=(global_shape, extra_dim_shape),\n                                partition=Partition.SCATTER,\n                                axis=0, mask=mask)\nx[:] = (MPI.COMM_WORLD.Get_rank() % subsize + 1.) * np.ones((local_shape, extra_dim_shape))\nxloc = x.asarray()\n\n# Dot product\ndot = x.dot(x)\ndotloc = np.dot(xloc[local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)].ravel(),\n                xloc[local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)].ravel())\nprint(f\"Dot check (Rank {rank}): {np.allclose(dot, dotloc)}\")\n\n# Norm\nnorm = x.norm(ord=2, axis=0)\nnormloc = np.linalg.norm(xloc[local_shape * subsize * (rank // subsize):local_shape * subsize * (rank // subsize + 1)],\n                         ord=2, axis=0)\nprint(f\"Norm check (Rank {rank}): {np.allclose(norm, normloc)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}