
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/plot_distributed_array.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_plot_distributed_array.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_plot_distributed_array.py:


Distributed Array
=========================
This example shows how to use the :py:class:`pylops_mpi.DistributedArray`.
This class provides a way to distribute arrays across multiple processes in
a parallel computing environment.

.. GENERATED FROM PYTHON SOURCE LINES 8-19

.. code-block:: default


    from matplotlib import pyplot as plt
    import numpy as np
    import pylops_mpi

    plt.close("all")
    np.random.seed(42)

    # Defining the global shape of the distributed array
    global_shape = (10, 5)








.. GENERATED FROM PYTHON SOURCE LINES 20-23

Let's start by defining the
class with the input parameters ``global_shape``,
``partition``, and ``axis``. Here's an example implementation of the class with ``axis=0``.

.. GENERATED FROM PYTHON SOURCE LINES 23-31

.. code-block:: default

    arr = pylops_mpi.DistributedArray(global_shape=global_shape,
                                      partition=pylops_mpi.Partition.SCATTER,
                                      axis=0)
    # Filling the local arrays
    arr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,
                       arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)
    pylops_mpi.plot_distributed_array(arr)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_001.png
   :alt: Original Array, Distributed over axis 0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 32-34

Below is an implementation to show how the global array is distributed along
the second axis.

.. GENERATED FROM PYTHON SOURCE LINES 34-42

.. code-block:: default

    arr = pylops_mpi.DistributedArray(global_shape=global_shape,
                                      partition=pylops_mpi.Partition.SCATTER,
                                      axis=1)
    # Filling the local arrays
    arr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,
                       arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)
    pylops_mpi.plot_distributed_array(arr)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_002.png
   :alt: Original Array, Distributed over axis 1
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 43-47

To convert a random NumPy array into a ``pylops_mpi.DistributedArray``,
you can use the ``to_dist`` classmethod. This method allows you to distribute
the array across multiple processes for parallel computation.
Below is an example implementation depicting the same.

.. GENERATED FROM PYTHON SOURCE LINES 47-57

.. code-block:: default

    n = global_shape[0] * global_shape[1]
    # Array to be distributed
    array = np.arange(n) / float(n)
    arr1 = pylops_mpi.DistributedArray.to_dist(x=array.reshape(global_shape), axis=1)
    array = array / 2.0
    arr2 = pylops_mpi.DistributedArray.to_dist(x=array.reshape(global_shape), axis=1)
    # plot local arrays
    pylops_mpi.plot_local_arrays(arr1, "Distributed Array - 1", vmin=0, vmax=1)
    pylops_mpi.plot_local_arrays(arr2, "Distributed Array - 2", vmin=0, vmax=1)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_003.png
         :alt: Distributed Array - 1, Rank-0
         :srcset: /gallery/images/sphx_glr_plot_distributed_array_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_004.png
         :alt: Distributed Array - 2, Rank-0
         :srcset: /gallery/images/sphx_glr_plot_distributed_array_004.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 58-60

**Element-wise Addition** - Each process operates on its local portion of
the array and adds the corresponding elements together.

.. GENERATED FROM PYTHON SOURCE LINES 60-63

.. code-block:: default

    sum_arr = arr1 + arr2
    pylops_mpi.plot_local_arrays(sum_arr, "Addition", vmin=0, vmax=1)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_005.png
   :alt: Addition, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 64-66

**Element-wise Subtraction** - Each process operates on its local portion
of the array and subtracts the corresponding elements together.

.. GENERATED FROM PYTHON SOURCE LINES 66-69

.. code-block:: default

    diff_arr = arr1 - arr2
    pylops_mpi.plot_local_arrays(diff_arr, "Subtraction", vmin=0, vmax=1)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_006.png
   :alt: Subtraction, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 70-72

**Element-wise Multiplication** - Each process operates on its local portion
of the array and multiplies the corresponding elements together.

.. GENERATED FROM PYTHON SOURCE LINES 72-74

.. code-block:: default

    mult_arr = arr1 * arr2
    pylops_mpi.plot_local_arrays(mult_arr, "Multiplication", vmin=0, vmax=1)



.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_007.png
   :alt: Multiplication, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_007.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.269 seconds)


.. _sphx_glr_download_gallery_plot_distributed_array.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_distributed_array.py <plot_distributed_array.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_distributed_array.ipynb <plot_distributed_array.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
