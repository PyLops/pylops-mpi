
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/plot_distributed_array.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_plot_distributed_array.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_plot_distributed_array.py:


Distributed Array
=========================
This example shows how to use the :py:class:`pylops_mpi.DistributedArray`.
This class provides a way to distribute arrays across multiple processes in
a parallel computing environment.

.. GENERATED FROM PYTHON SOURCE LINES 8-22

.. code-block:: Python


    from matplotlib import pyplot as plt
    import numpy as np
    from mpi4py import MPI

    from pylops_mpi.DistributedArray import local_split, Partition
    import pylops_mpi

    plt.close("all")
    np.random.seed(42)

    # Defining the global shape of the distributed array
    global_shape = (10, 5)








.. GENERATED FROM PYTHON SOURCE LINES 23-26

Let's start by defining the
class with the input parameters ``global_shape``,
``partition``, and ``axis``. Here's an example implementation of the class with ``axis=0``.

.. GENERATED FROM PYTHON SOURCE LINES 26-34

.. code-block:: Python

    arr = pylops_mpi.DistributedArray(global_shape=global_shape,
                                      partition=pylops_mpi.Partition.SCATTER,
                                      axis=0)
    # Filling the local arrays
    arr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,
                       arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)
    pylops_mpi.plot_distributed_array(arr)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_001.png
   :alt: Original Array, Distributed over axis 0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 35-37

Below is an implementation to show how the global array is distributed along
the second axis.

.. GENERATED FROM PYTHON SOURCE LINES 37-45

.. code-block:: Python

    arr = pylops_mpi.DistributedArray(global_shape=global_shape,
                                      partition=pylops_mpi.Partition.SCATTER,
                                      axis=1)
    # Filling the local arrays
    arr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,
                       arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)
    pylops_mpi.plot_distributed_array(arr)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_002.png
   :alt: Original Array, Distributed over axis 1
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 46-51

You also have the option of directly including the ``local_shapes`` as a parameter
to the :py:class:`pylops_mpi.DistributedArray`. This will enable the assignment
of shapes to local arrays on each rank. However, it's essential to ensure that
the number of processes matches the length of ``local_shapes``, and that the
combined local shapes should align with the ``global_shape`` along the desired ``axis``.

.. GENERATED FROM PYTHON SOURCE LINES 51-59

.. code-block:: Python

    local_shape = local_split(global_shape, MPI.COMM_WORLD, Partition.SCATTER, 0)
    # Assigning local_shapes(List of tuples)
    local_shapes = MPI.COMM_WORLD.allgather(local_shape)
    arr = pylops_mpi.DistributedArray(global_shape=global_shape, local_shapes=local_shapes, axis=0)
    arr[:] = np.arange(arr.local_shape[0] * arr.local_shape[1] * arr.rank,
                       arr.local_shape[0] * arr.local_shape[1] * (arr.rank + 1)).reshape(arr.local_shape)
    pylops_mpi.plot_distributed_array(arr)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_003.png
   :alt: Original Array, Distributed over axis 0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 60-64

To convert a random NumPy array into a ``pylops_mpi.DistributedArray``,
you can use the ``to_dist`` classmethod. This method allows you to distribute
the array across multiple processes for parallel computation.
Below is an example implementation depicting the same.

.. GENERATED FROM PYTHON SOURCE LINES 64-74

.. code-block:: Python

    n = global_shape[0] * global_shape[1]
    # Array to be distributed
    array = np.arange(n) / float(n)
    arr1 = pylops_mpi.DistributedArray.to_dist(x=array.reshape(global_shape), axis=1)
    array = array / 2.0
    arr2 = pylops_mpi.DistributedArray.to_dist(x=array.reshape(global_shape), axis=1)
    # plot local arrays
    pylops_mpi.plot_local_arrays(arr1, "Distributed Array - 1", vmin=0, vmax=1)
    pylops_mpi.plot_local_arrays(arr2, "Distributed Array - 2", vmin=0, vmax=1)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_004.png
         :alt: Distributed Array - 1, Rank-0
         :srcset: /gallery/images/sphx_glr_plot_distributed_array_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_005.png
         :alt: Distributed Array - 2, Rank-0
         :srcset: /gallery/images/sphx_glr_plot_distributed_array_005.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 75-77

**Scaling** - Each process operates on its local portion of
the array and scales the corresponding elements by a given scalar.

.. GENERATED FROM PYTHON SOURCE LINES 77-80

.. code-block:: Python

    scale_arr = .5 * arr1
    pylops_mpi.plot_local_arrays(scale_arr, "Scaling", vmin=0, vmax=1)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_006.png
   :alt: Scaling, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 81-83

**Element-wise Addition** - Each process operates on its local portion of
the array and adds the corresponding elements together.

.. GENERATED FROM PYTHON SOURCE LINES 83-86

.. code-block:: Python

    sum_arr = arr1 + arr2
    pylops_mpi.plot_local_arrays(sum_arr, "Addition", vmin=0, vmax=1)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_007.png
   :alt: Addition, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_007.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 87-90

**Element-wise In-place Addition** - Similar to the previous one but the
addition is performed directly on one of the addends without creating a new
distributed array.

.. GENERATED FROM PYTHON SOURCE LINES 90-93

.. code-block:: Python

    sum_arr += arr2
    pylops_mpi.plot_local_arrays(sum_arr, "Addition", vmin=0, vmax=1)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_008.png
   :alt: Addition, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_008.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 94-96

**Element-wise Subtraction** - Each process operates on its local portion
of the array and subtracts the corresponding elements together.

.. GENERATED FROM PYTHON SOURCE LINES 96-99

.. code-block:: Python

    diff_arr = arr1 - arr2
    pylops_mpi.plot_local_arrays(diff_arr, "Subtraction", vmin=0, vmax=1)




.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_009.png
   :alt: Subtraction, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_009.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 100-102

**Element-wise Multiplication** - Each process operates on its local portion
of the array and multiplies the corresponding elements together.

.. GENERATED FROM PYTHON SOURCE LINES 102-104

.. code-block:: Python

    mult_arr = arr1 * arr2
    pylops_mpi.plot_local_arrays(mult_arr, "Multiplication", vmin=0, vmax=1)



.. image-sg:: /gallery/images/sphx_glr_plot_distributed_array_010.png
   :alt: Multiplication, Rank-0
   :srcset: /gallery/images/sphx_glr_plot_distributed_array_010.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.505 seconds)


.. _sphx_glr_download_gallery_plot_distributed_array.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_distributed_array.ipynb <plot_distributed_array.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_distributed_array.py <plot_distributed_array.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_distributed_array.zip <plot_distributed_array.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
